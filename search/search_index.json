{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vertex pipelines starter kit","text":"<p>This repository template is meant to help Vertex pipelines beginners understand and avoid its pitfalls. It illustrates what we find is the most efficient way to transition from notebook exploration to a reliable, industrialized ML pipeline.</p> <p>It is based on the collective knowledge and feedbacks of past Artefact projects and people. It is meant to illustrate an architecture that provides both iteration/exploration speed, as well as reliability for industrialisation.</p> <p>It also touches on patterns that we have seen not work well or had worse velocity and reliability in our experience.</p>"},{"location":"#understanding-the-vertex-ai-platform","title":"Understanding the Vertex AI platform","text":"<p>Before we start, it is important to mention the difference between Vertex AI and Vertex AI Pipelines. Vertex AI is a set of services gathered in a unified platform to accelerate data science projects: Vertex AI Dataset, Vertex AI Pipelines, Vertex AI endpoints, etc... Vertex AI Pipelines is one of the service included in the Vertex AI platform.</p>"},{"location":"#vertex-pipelines","title":"Vertex pipelines","text":"<p>So, what are Vertex AI Pipelines?</p> <p>Vertex AI Pipelines is a managed service offered by GCP to execute Kubeflow Pipelines.</p> <p>If you separate all your machine learning workflow into \u201ccomponents\u201d representing each steps of your ML workflow, then you can write your workflow using Kubeflow Pipelines SDK. In return, Kubeflow Pipeline makes your workflow portable and scalable.</p> <p>Vertex AI Pipeline is a managed service to run your Kubeflow Pipelines.</p>"},{"location":"#convictions","title":"Convictions","text":"<p>Kubeflow pipelines is a very powerful tool, but it is also very complex. On top of that you will be using it in a managed environment, which steepens the learning curve. To make developping vertex pipelines less painful, our main recommandation is to structure your code in a way that will allow you to work just like you would in a regular python project, and only use Vertex pipelines for orchestration and nothing else.</p> <p>We want a strict eparation of concerns between the application code that does the data science, and the Vertex pipeline that will run it in an industrialized context.</p> <p>On top of that, there are two big issues that we address with this starter kit:</p> <ul> <li>Vertex Pipelines have a high execution time overhead. Your code in a pipeline component will have an incompressible 2 minutes startup time to provision resources. While it does not look like a lot, it absolutely kills development velocity and cycle time. You can not iterate quickly with a 2 minutes overhead for each execution.</li> <li>Vertex Pipelines are very flexible and there are many ways to develop with them. Too many. Here we try to provide one way of working with the tool that reduces the pain of industrialisation while preserving iteration speed during the exploration, development, and evolution of your ML pipeline.</li> </ul> <p>Here are our recommendation to avoid the problems above:</p> <ul> <li>Use a single base docker image for all components. This is the environement your code will run in when in a pipeline. Don't lose time creating a tailor-made one for each component. Save time by having the same image with all the required dependencies and use it everywhere.</li> <li>Only use function-based components. Container-based components are much harder to build, deploy, and maintain. Use the slimmest possible function-based components to reduce your operational load.</li> <li>Embark as little intelligence as possible in the components. Enforcing a strong separation of concerns between your business and orchestration logic will allow you handle these two functionalities separately, making debugging easier and faster. </li> <li>Have the business logic concentrated in a regular python folder/files structure. This will facilitate the separation of concerns, allow you to run your code locally, unit-test it easily, etc...</li> <li>This code should be locally executable. Essential to be able to quickly iterate on it.</li> </ul>"},{"location":"#process-for-working-with-vertex-pipelines","title":"Process for working with Vertex pipelines","text":"<p>Here we describe a workflow we have tested and recommend when using vertex pipelines.  An example of following this workflow is included in this repository (final pipeline output is <code>vertex/pipelines/my_first_pipeline.py</code> )</p> <p>Note that if you are not a fan of jupyter notebooks you can easily replace notebooks with scripts for a similar workflow</p>"},{"location":"#phase-1-notebook-exploration","title":"Phase 1 - Notebook exploration","text":"<ul> <li>Do your usual notebook things, maybe start thinking about writing functions, but no pressure.</li> </ul>"},{"location":"#phase-2-refactor-your-notebook-for-it-to-look-like-a-pipeline","title":"Phase 2 - Refactor your notebook for it to look like a pipeline","text":"<ul> <li>Wrap code in functions to make them easily transferable to scripts</li> <li>Write your Notebook in a way that will mirror an ML pipeline: a section for a pipeline step/component, the notebook itself could represent the pipeline.</li> <li>When loading data from files, or tables to pandas dataframe, make sure to explicitly cast to the types that you will use (for cases which are ambiguous). Vertex has a way of loading data that may produce similar but different data types.</li> <li>Type hint functions inputs/outputs as a general good practice and to identify at a glance the vertex-compatible ones</li> </ul>"},{"location":"#phase-3-move-your-code-to-libraries","title":"Phase 3 - Move your code to libraries","text":"<ul> <li>Migrate main and subfunctions to a <code>lib</code> folder. </li> <li>Those functions will be called both: </li> <li>In your notebook for iterating and testing new features</li> <li>By your vertex components for running the pipelines</li> <li>At this point your notebook should only include import, some config definition and application of your main functions.</li> <li>Have clear entrypoint functions to then use in components</li> <li>Maybe even write a few unit/integration tests</li> <li>Have a fully functional workflow before even thinking of moving to Vertex Pipelines</li> <li><code>notebooks/vertex_pipelines/my_first_pipeline.ipynb</code> gives an example of how do this for a simple ETL</li> </ul> <p>Example of pipeline notebook for a simple ETL (each section is a component): </p>"},{"location":"#phase-4-wrap-this-code-in-components-and-pipelines","title":"Phase 4 - Wrap this code in components and pipelines","text":"<ul> <li>Now we are finally doing some vertex ! </li> <li>Write your function based components, you should only load / save your data (as this is vertex specific code) and import and call your top-level functions. Again, if you find yourself writing business or technical logic in the component, you should think about moving it to your <code>lib</code></li> <li>Finally, you can compose your pipeline from the components that you defined.</li> <li>Run your pipeline, and hope for the best.</li> </ul> <p>transform_data Component</p> <pre><code>@component(base_image=f'europe-west1-docker.pkg.dev/{os.getenv(\"PROJECT_ID\")}/vertex-pipelines-docker/vertex-pipelines-base:latest')\ndef transform_data_component(\n    df: Input[Dataset],\n    column_name: str,\n    constant_value: str,\n    df_transformed_dataset: Output[Dataset]\n):\n    import pandas as pd\n    from vertex.lib.processors.transform_data import add_constant_column\n\n    df = pd.read_csv(df.uri)\n    df_transformed = add_constant_column(df, column_name, constant_value)\n    df_transformed.to_csv(df_transformed_dataset.uri, index=False)\n</code></pre>"},{"location":"#phase-5-iterate-and-improve","title":"Phase 5 - Iterate and improve","text":"<p>You are probably going to need to go back to phase 1 to iterate and go through the motions again, from notebook to running pipeline. This is expected and normal. Doing it often will make you think end-to-end and allow you to close the feedback loop with users, thus limiting tunnel effect. It will also highlight potential integration risks within and around the pipeline early on. See: Walking skeleton approach.</p> <p>It is a very good idea to keep maintaining the notebooks you used for creating a pipeline so that they can be used for quickly updating existing pipeline :  modify your libs test in the notebook and once everything is working, test with vertex. </p>"},{"location":"base_image/","title":"Docker base image for pipelines","text":"<p>Vertex pipelines will execute your components in a Docker image that you will need to specify. We recommend building a single base image that will embark all your python packages and codebase. You could have as many images as you have components, each taylored and optimized to only embark the code and dependencies that you need, but that is excessively difficult and much less efficient to develop, deploy, and maintain.</p>"},{"location":"base_image/#use-a-single-base-image","title":"Use a single base image","text":"<p>Having a unique base image that serves as an execution environment for all your code and components has several advantages:</p> <ul> <li>The base image is extremely simple, and you will only ever have to write the Dockerfile once.</li> <li>You will only need one CI/CD to build and push that image to your image registry.</li> <li>It's easier to have a local environement identical to your image, reducing the risk of code working locally, but not in the pipeline.</li> <li>All your components will have the same boilerplate decorator, avoiding copy-paste errors and making the codebase more consistent.</li> </ul>"},{"location":"base_image/#exceptions-to-the-rule","title":"Exceptions to the rule","text":"<p>There are certain cases when it can be better to use a separate image for a specific operator.</p> <ul> <li>Operators that require a package that is not compatible with the rest of your base image. Maybe you need a different runtime than python, or you need an old version of a package that conflicts with the rest of your dependencies.</li> <li>Very large dependencies that would bloat the rest of your pipeline. If a dependency adds entire gigabytes to your base image, then it could be a good reason to dedicate another image for it. Even then, prefer the method described here to add extra packages to a specific component.</li> </ul> <p>These cases should be very rare. Even if you feel like 90% of what you put in the base image is unused by any single component, it does not cost anything for it to be there anyways. What is costly however, is building a new dedicated image which will require you to write a new Dockerfile, CI/CD, run tests, write doc, onboard others, and maintain it.</p>"},{"location":"howto_CPU_RAM_resources/","title":"How to allocate more RAM and CPU in a pipeline?","text":"<p>In the following example, <code>flip_coin</code> is a component object.</p> <pre><code>@dsl.pipeline\ndef my_pipeline():\n    coin_flip_task = flip_coin() \\\n        .set_cpu_limit(8) \\\n        .set_memory_limit(\"16G\") \\\n        .add_node_selector_constraint(\"NVIDIA_TESLA_K80\") \\\n        .set_gpu_limit(2)\n</code></pre> <p>Reference: https://cloud.google.com/vertex-ai/docs/pipelines/machine-types</p>"},{"location":"howto_act_on_failure/","title":"Handling Vertex AI Pipelines Failures","text":"<p>In Vertex AI Pipelines, failures can occur during the execution of a pipeline. To handle these failures, you can use Exit Handlers.</p>"},{"location":"howto_act_on_failure/#exit-handlers","title":"Exit Handlers","text":"<p>Official documentation for Exit Handlers is available here.</p> <p>Exit Handlers are special steps that are executed whenever a pipeline exits, regardless of whether the pipeline\u2019s steps have completed successfully or have failed. You can use them to  send notifications, or perform any other action that you want to occur when a pipeline exits with a certain status (e.g. failure).</p> <p>Here is an example of how to use Exit Handlers:</p> <pre><code>import kfp.dsl\n\n@kfp.dsl.pipeline(name=\"my-pipeline\")\ndef my_pipeline():\n\n    my_exit_task = ...  # Define your exit task here. (instanciated component)\n\n    with kfp.dsl.ExitHandler(my_exit_task, name=\"Exit Handler\"):\n        # Add your pipeline tasks here.\n</code></pre>"},{"location":"howto_act_on_failure/#sending-notifications","title":"Sending Notifications","text":""},{"location":"howto_act_on_failure/#email-notification-using-vertexnotificationemailop","title":"Email notification using <code>VertexNotificationEmailOp</code>","text":"<p>Official documentation for this component is available here.</p> <p>Vertex AI Pipelines provides a built-in component to send email notifications. This component is called <code>VertexNotificationEmailOp</code>. It is available in the <code>google_cloud_pipeline_components</code> package. You can use it as an exit handler in your pipeline to send email notifications when the pipeline exits.</p> <pre><code>import kfp.dsl\nfrom google_cloud_pipeline_components.v1.vertex_notification_email import VertexNotificationEmailOp\n\n@kfp.dsl.pipeline(name=\"my-pipeline\")\ndef my_pipeline():\n\n    notification_email_task = VertexNotificationEmailOp(recipients=[\"john.doe@foo-bar.com\"])\n\n    with kfp.dsl.ExitHandler(notification_email_task, name=\"Exit Handler\"):\n        # Add your pipeline tasks here.\n</code></pre>"},{"location":"howto_act_on_failure/#slack-notification-using-custom-component","title":"Slack notification using custom component","text":"<p>You can also use a custom component to send notifications. Here is an example of a custom component that sends a notification to a Slack channel when a pipeline job ends.</p> <p>Prerequisites</p> <p>To use this component, you need to:</p> <ul> <li>Create a Slack app and a Slack webhook URL. Official documentation.</li> <li>Create a secret and secret version in Secret Manager containing the Slack webhook URL. Official documentation.</li> </ul> 1. Create message to be sent to Slack2. Retrieve Slack webhook URL from Secret Manager3. Send message or log message <p>Message creation includes:</p> <ul> <li>Retrieving information about the job</li> <li>Creating the string template</li> <li>Creating the JSON payload to be sent to Slack</li> </ul> <pre><code>from typing import Optional\n\nfrom kfp.dsl import PipelineTaskFinalStatus, component\n\n\n@component(\n    base_image=\"python:3.10-slim-buster\",\n    packages_to_install=[\"google-cloud-aiplatform\", \"requests\", \"google-cloud-secret-manager\"],\n)\ndef vertex_pipelines_notification_slack(\n    project_id: str,\n    pipeline_task_final_status: PipelineTaskFinalStatus,\n    slack_webhook_url_secret_name: Optional[str] = None,\n):\n    \"\"\"KFP Component that sends a notification to a Slack channel when a pipeline job ends.\n    To be used as an exit handler in a pipeline.\n\n    Args:\n        pipeline_task_final_status (PipelineTaskFinalStatus): The status of the pipeline job.\n        slack_webhook_url_secret_name (str, optional): The name of the secret containing the\n            Slack webhook URL. Defaults to None. If None or empty string, the notification will\n            be printed to stdout instead of being sent to Slack.\n    \"\"\"\n    import logging\n    from zoneinfo import ZoneInfo\n\n    import requests\n    from google.cloud import aiplatform\n    from google.cloud.secretmanager import SecretManagerServiceClient\n\n    logging.getLogger().setLevel(logging.INFO)\n    logging.basicConfig(format=\"%(levelname)s: %(message)s\", level=logging.INFO)\n\n    pipeline_job = aiplatform.PipelineJob.get(\n        resource_name=pipeline_task_final_status.pipeline_job_resource_name\n    )\n\n    emojis = {\n        \"SUCCEEDED\": [\"\u2705\", \":risibeer:\"],\n        \"FAILED\": [\"\u274c\", \":risicry:\"],\n        \"CANCELLED\": [\"\ud83d\udeab\", \"\ud83d\udeab\"],\n    }\n\n    TIMEZONE = \"Europe/Paris\"\n\n    status = pipeline_task_final_status.state\n    project = pipeline_task_final_status.pipeline_job_resource_name.split(\"/\")[1]\n    pipeline_name = pipeline_task_final_status.pipeline_job_resource_name.split(\"/\")[5]\n    pipeline_job_id = pipeline_task_final_status.pipeline_job_resource_name\n    start_time = (\n        f\"{pipeline_job.create_time.astimezone(tz=ZoneInfo(TIMEZONE)).isoformat()} {TIMEZONE}\"\n    )\n    console_link = pipeline_job._dashboard_uri()\n\n    title_str = f\"{emojis[status][0]} Vertex Pipelines job *{pipeline_name}* ended with the following state: *{status}* {emojis[status][1]}.\"  # noqa: E501\n\n    additional_details = f\"\"\"*Additional details:*\n    - *Project:* {project}\n    - *Pipeline name:* {pipeline_name}\n    - *Pipeline job ID:* {pipeline_job_id}\n    - *Start time:* {start_time}\n\nTo view this pipeline job in Cloud Console, use the following link: {console_link}\n\"\"\"\n\n    notification_json = {\n        \"blocks\": [\n            {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": title_str}},\n            {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": additional_details}},\n        ]\n    }\n\n    def get_secret_value(project_id, secret_name) -&gt; str:\n        client = SecretManagerServiceClient()\n        name = f\"projects/{project_id}/secrets/{secret_name}/versions/latest\"\n        response = client.access_secret_version(name=name)\n        return response.payload.data.decode(\"UTF-8\")\n\n    if slack_webhook_url_secret_name:\n        slack_webhook_url = get_secret_value(project_id, slack_webhook_url_secret_name)\n\n        response = requests.post(\n            slack_webhook_url,\n            json=notification_json,\n            headers={\"Content-type\": \"application/json\"},\n        )\n\n        response.raise_for_status()\n    else:\n        logging.info(title_str + \"\\n\" + additional_details)\n</code></pre> <p>If the Slack webhook URL secret name is provided, the Slack webhook URL is retrieved from Secret Manager.</p> <pre><code>from typing import Optional\n\nfrom kfp.dsl import PipelineTaskFinalStatus, component\n\n\n@component(\n    base_image=\"python:3.10-slim-buster\",\n    packages_to_install=[\"google-cloud-aiplatform\", \"requests\", \"google-cloud-secret-manager\"],\n)\ndef vertex_pipelines_notification_slack(\n    project_id: str,\n    pipeline_task_final_status: PipelineTaskFinalStatus,\n    slack_webhook_url_secret_name: Optional[str] = None,\n):\n    \"\"\"KFP Component that sends a notification to a Slack channel when a pipeline job ends.\n    To be used as an exit handler in a pipeline.\n\n    Args:\n        pipeline_task_final_status (PipelineTaskFinalStatus): The status of the pipeline job.\n        slack_webhook_url_secret_name (str, optional): The name of the secret containing the\n            Slack webhook URL. Defaults to None. If None or empty string, the notification will\n            be printed to stdout instead of being sent to Slack.\n    \"\"\"\n    import logging\n    from zoneinfo import ZoneInfo\n\n    import requests\n    from google.cloud import aiplatform\n    from google.cloud.secretmanager import SecretManagerServiceClient\n\n    logging.getLogger().setLevel(logging.INFO)\n    logging.basicConfig(format=\"%(levelname)s: %(message)s\", level=logging.INFO)\n\n    pipeline_job = aiplatform.PipelineJob.get(\n        resource_name=pipeline_task_final_status.pipeline_job_resource_name\n    )\n\n    emojis = {\n        \"SUCCEEDED\": [\"\u2705\", \":risibeer:\"],\n        \"FAILED\": [\"\u274c\", \":risicry:\"],\n        \"CANCELLED\": [\"\ud83d\udeab\", \"\ud83d\udeab\"],\n    }\n\n    TIMEZONE = \"Europe/Paris\"\n\n    status = pipeline_task_final_status.state\n    project = pipeline_task_final_status.pipeline_job_resource_name.split(\"/\")[1]\n    pipeline_name = pipeline_task_final_status.pipeline_job_resource_name.split(\"/\")[5]\n    pipeline_job_id = pipeline_task_final_status.pipeline_job_resource_name\n    start_time = (\n        f\"{pipeline_job.create_time.astimezone(tz=ZoneInfo(TIMEZONE)).isoformat()} {TIMEZONE}\"\n    )\n    console_link = pipeline_job._dashboard_uri()\n\n    title_str = f\"{emojis[status][0]} Vertex Pipelines job *{pipeline_name}* ended with the following state: *{status}* {emojis[status][1]}.\"  # noqa: E501\n\n    additional_details = f\"\"\"*Additional details:*\n    - *Project:* {project}\n    - *Pipeline name:* {pipeline_name}\n    - *Pipeline job ID:* {pipeline_job_id}\n    - *Start time:* {start_time}\n\nTo view this pipeline job in Cloud Console, use the following link: {console_link}\n\"\"\"\n\n    notification_json = {\n        \"blocks\": [\n            {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": title_str}},\n            {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": additional_details}},\n        ]\n    }\n\n    def get_secret_value(project_id, secret_name) -&gt; str:\n        client = SecretManagerServiceClient()\n        name = f\"projects/{project_id}/secrets/{secret_name}/versions/latest\"\n        response = client.access_secret_version(name=name)\n        return response.payload.data.decode(\"UTF-8\")\n\n    if slack_webhook_url_secret_name:\n        slack_webhook_url = get_secret_value(project_id, slack_webhook_url_secret_name)\n\n        response = requests.post(\n            slack_webhook_url,\n            json=notification_json,\n            headers={\"Content-type\": \"application/json\"},\n        )\n\n        response.raise_for_status()\n    else:\n        logging.info(title_str + \"\\n\" + additional_details)\n</code></pre> <p>Depending on whether the Slack Webhook URL secret name is provided, the message will be sent to Slack or logged to stdout. This is useful for testing purposes, to avoid being spammed with notifications.</p> <pre><code>from typing import Optional\n\nfrom kfp.dsl import PipelineTaskFinalStatus, component\n\n\n@component(\n    base_image=\"python:3.10-slim-buster\",\n    packages_to_install=[\"google-cloud-aiplatform\", \"requests\", \"google-cloud-secret-manager\"],\n)\ndef vertex_pipelines_notification_slack(\n    project_id: str,\n    pipeline_task_final_status: PipelineTaskFinalStatus,\n    slack_webhook_url_secret_name: Optional[str] = None,\n):\n    \"\"\"KFP Component that sends a notification to a Slack channel when a pipeline job ends.\n    To be used as an exit handler in a pipeline.\n\n    Args:\n        pipeline_task_final_status (PipelineTaskFinalStatus): The status of the pipeline job.\n        slack_webhook_url_secret_name (str, optional): The name of the secret containing the\n            Slack webhook URL. Defaults to None. If None or empty string, the notification will\n            be printed to stdout instead of being sent to Slack.\n    \"\"\"\n    import logging\n    from zoneinfo import ZoneInfo\n\n    import requests\n    from google.cloud import aiplatform\n    from google.cloud.secretmanager import SecretManagerServiceClient\n\n    logging.getLogger().setLevel(logging.INFO)\n    logging.basicConfig(format=\"%(levelname)s: %(message)s\", level=logging.INFO)\n\n    pipeline_job = aiplatform.PipelineJob.get(\n        resource_name=pipeline_task_final_status.pipeline_job_resource_name\n    )\n\n    emojis = {\n        \"SUCCEEDED\": [\"\u2705\", \":risibeer:\"],\n        \"FAILED\": [\"\u274c\", \":risicry:\"],\n        \"CANCELLED\": [\"\ud83d\udeab\", \"\ud83d\udeab\"],\n    }\n\n    TIMEZONE = \"Europe/Paris\"\n\n    status = pipeline_task_final_status.state\n    project = pipeline_task_final_status.pipeline_job_resource_name.split(\"/\")[1]\n    pipeline_name = pipeline_task_final_status.pipeline_job_resource_name.split(\"/\")[5]\n    pipeline_job_id = pipeline_task_final_status.pipeline_job_resource_name\n    start_time = (\n        f\"{pipeline_job.create_time.astimezone(tz=ZoneInfo(TIMEZONE)).isoformat()} {TIMEZONE}\"\n    )\n    console_link = pipeline_job._dashboard_uri()\n\n    title_str = f\"{emojis[status][0]} Vertex Pipelines job *{pipeline_name}* ended with the following state: *{status}* {emojis[status][1]}.\"  # noqa: E501\n\n    additional_details = f\"\"\"*Additional details:*\n    - *Project:* {project}\n    - *Pipeline name:* {pipeline_name}\n    - *Pipeline job ID:* {pipeline_job_id}\n    - *Start time:* {start_time}\n\nTo view this pipeline job in Cloud Console, use the following link: {console_link}\n\"\"\"\n\n    notification_json = {\n        \"blocks\": [\n            {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": title_str}},\n            {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": additional_details}},\n        ]\n    }\n\n    def get_secret_value(project_id, secret_name) -&gt; str:\n        client = SecretManagerServiceClient()\n        name = f\"projects/{project_id}/secrets/{secret_name}/versions/latest\"\n        response = client.access_secret_version(name=name)\n        return response.payload.data.decode(\"UTF-8\")\n\n    if slack_webhook_url_secret_name:\n        slack_webhook_url = get_secret_value(project_id, slack_webhook_url_secret_name)\n\n        response = requests.post(\n            slack_webhook_url,\n            json=notification_json,\n            headers={\"Content-type\": \"application/json\"},\n        )\n\n        response.raise_for_status()\n    else:\n        logging.info(title_str + \"\\n\" + additional_details)\n</code></pre> <p>Then you can use this component as an exit handler in your pipeline:</p> <pre><code>import kfp.dsl\n\nfrom my_custom_components import vertex_pipelines_notification_slack\n\n@kfp.dsl.pipeline(name=\"my-pipeline\")\ndef my_pipeline(project_id: str, slack_webhook_url_secret_name: str):\n    notification_task = vertex_pipelines_notification_slack(\n        project_id=project_id,\n        slack_webhook_url_secret_name=slack_webhook_url_secret_name\n    )\n\n    with kfp.dsl.ExitHandler(notification_task, name=\"Exit Handler\"):\n        # Add your pipeline tasks here.\n</code></pre>"},{"location":"howto_bentoml/","title":"How to deploy a BentoML bundle to VertexAI","text":"<p>BentoML is a library that allows you to build the online serving API to serve your model. The purpose of this tutorial is to show you how to deploy a BentoML bundle to a VertexAI Endpoint.</p> <p>Along with this tutorial, an example is provided, with some useful function to perform this process in a VertexAI pipeline. Check it out on Github</p>"},{"location":"howto_bentoml/#table-of-contents","title":"Table of contents","text":"<ul> <li>How to deploy a BentoML bundle to VertexAI</li> <li>Table of contents</li> <li>Should you use BentoML?</li> <li>Key ressources</li> <li>Steps<ul> <li>1. Save the model to BentoML registry</li> <li>2. Create the API service</li> <li>3. Write the bentofile.yaml file</li> <li>4. Build the Docker image</li> <li>5. Testing the service locally</li> <li>6. Upload the model to Google Artifact Regitry (GAR)</li> <li>7.Import image to VertexAI model registry</li> <li>8.Deploy model to VertexAI endpoint</li> <li>9.Test the endpoint</li> </ul> </li> </ul>"},{"location":"howto_bentoml/#should-you-use-bentoml","title":"Should you use BentoML?","text":"<p>When not using the build-in algorithms, model deployment on VertexAI requires users to build their own container image and API server. BentoML is a library that allows you to turn your ML model into production API endpoint with just a few lines of code. It will handle the creation of the serving API and the Docker image. It can be an alternative to Tensorflow Serving or TorchServe.</p> <p>Quote</p> <p>Shipping ML models to production is broken. Data Scientists may not have all the expertise in building production services and the trained models they delivered are very hard to test and deploy. This often leads to a time consuming and error-prone workflow, where a pickled model or weights file is handed over to a software engineering team. BentoML is an end-to-end solution for model serving, making it possible for Data Science teams to ship their models as prediction services, in a way that is easy to test, easy to deploy, and easy to integrate with other DevOps tools.</p> <p>Please read these ressources to go further:</p> <ul> <li>Reddit thread</li> <li>Official documentation</li> </ul>"},{"location":"howto_bentoml/#key-ressources","title":"Key ressources","text":"<p>This tutorial assume you already know the basics of BentoML. Please read these ressources first:</p> <ul> <li>Quickstart BentoML Latest version &gt;1.0</li> <li>Quicktart &lt;0.3 LTS</li> <li>Deploying on VertexAI Workbench &lt;0.3 LTS</li> <li>GCP - How to deploy PyTorch models on Vertex AI using TorchServe</li> </ul>"},{"location":"howto_bentoml/#steps","title":"Steps","text":"<p>Here are the steps to deploy a BentoML bundle to VertexAI:</p> <ol> <li>Save the model to BentoML registry</li> <li>Create the API service</li> <li>Write the bentofile.yaml file</li> <li>Build the Docker image</li> <li>Testing the service locally</li> <li>Upload the model to Google Artifact Regitry (GAR)</li> <li>Import image to VertexAI model registry</li> <li>Deploy model to VertexAI endpoint</li> <li>Test the endpoint</li> </ol> <p>Steps 1 to 5 are pure BentoML development steps. Here is a high-level overview of what they do:</p> <ol> <li>Save the model to BentoML registry: In this step, you save the trained model from your ML framework (scikit, pytorch) to a BentoML model registry.</li> <li>Create the API service: Create a <code>service.py</code> file to wrap your model and lay out the serving logic.</li> <li>Write the bentofile.yaml file: Package your model and the BentoML Service into a Bento through a configuration YAML file. Each Bento corresponds to a directory that contains all the source code, dependencies, and model files required to serve the Bento, and an auto-generated Dockerfile for containerization.</li> <li>Build the Docker image: This will build the Docker image and push it.</li> <li>Testing the service locally: In this step, you test that the prediction service works locally.</li> </ol> <p>If you want to understand what is done at these steps, read the BentoML quick start.</p> <p>In steps 6 to 9, we will deploy the serving API to VertexAI:</p> <ol> <li>Upload the model to Google Artifact Regitry (GAR): This will upload the Docker image of the serving API to Google Artifact Regitry.</li> <li>Import image to VertexAI model registry: Import the Docker Image as a custom model in Vertex AI model registry.</li> <li>Deploy model to VertexAI endpoint: Deploy the model from the registry to an online-prediction endpoint on VertexAI.</li> <li>Test the endpoint: Send a request to the VertexAI endpoint to test it.</li> </ol>"},{"location":"howto_bentoml/#1-save-the-model-to-bentoml-registry","title":"1. Save the model to BentoML registry","text":"<p>Here is an example using Sklearn, but bentoml supports many frameworks.</p> bin/save_model.py<pre><code>from sklearn import svm, datasets\nimport bentoml\n\nMODEL_NAME = \"iris_clf\"\n\n# Load training data\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\n# Model Training\nclf = svm.SVC()\nclf.fit(X, y)\n\n# Save the model to BentoML format\nsaved_model = bentoml.sklearn.save_model(MODEL_NAME, clf)\nprint(saved_model.path)\n</code></pre> <p>Once the model is saved, you can access it thought the CLI command: <code>bentoml models get iris_clf:latest</code></p>"},{"location":"howto_bentoml/#2-create-the-api-service","title":"2. Create the API service","text":"<p>One specificity here is that VertexAI endpoints only accept JSON input and output, formated in a specific way. Inputs must be formated as a JSON object with a key \"instances\" containing a list of lists of values. Outputs must be formated as a JSON object with a key \"predictions\" containing a list of values.</p> <p>Example of input:</p> query.json<pre><code>{\n  \"instances\": [\n    [5.1, 3.5, 1.4, 0.2],\n    [4.9, 3.0, 1.4, 0.2]\n  ]\n}\n</code></pre> <p>Example of response:</p> <pre><code>{\n  \"predictions\": [\n    \"setosa\",\n    \"setosa\"\n  ]\n}\n</code></pre> <p>So we need to write a service that will take the input, format it to a list of lists, and then call the BentoML bundle. Then, we need to format the output to the VertexAI format.</p> <p>You can use pydantic to validate the input and output of your API. This will also enrich the API Swagger documentation that is automatically generated.</p> service.py<pre><code>import bentoml\nfrom bentoml.io import JSON\nfrom pydantic import BaseModel\nfrom typing import List\n\n# write data model in a separate file for better code readability\nclass Query(BaseModel):\n    instances: List[List[float]]\n\nclass Response(BaseModel):\n    predictions: List[str]\n\n# Load the BentoML bundle\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\ninput_schema = JSON(pydantic_model=Query)\noutput_schema = JSON(pydantic_model=Response)\n\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n@svc.api(input=input_schema, output=output_schema)\ndef classify(input_series: dict) -&gt; dict:\n    input_series = input_series[\"instances\"]\n    return {\"predictions\": iris_clf_runner.predict.run(input_series)}\n</code></pre> <p>You can test the service locally using the following command: <code>bentoml serve service.py:svc --reload</code></p>"},{"location":"howto_bentoml/#3-write-the-bentofileyaml-file","title":"3. Write the bentofile.yaml file","text":"bentofile.yaml<pre><code>service: \"service.py:svc\"\nlabels:\n  owner: bentoml-team\n  project: gallery\ninclude:\n  - \"*.py\"\npython:\n  packages:\n    - scikit-learn\n    - pandas\n</code></pre>"},{"location":"howto_bentoml/#4-build-the-docker-image","title":"4. Build the Docker image","text":"<p>Here are the steps to build the Docker image and run it locally:</p> <pre><code>VERSION=\"0.0.1\"\nGCP_PROJECT=\"my-gcp-project\"\nSERVICE=\"iris_classifier\"\nYAML_PATH=\"bentofile.yaml\"\nIMAGE_URI=eu.gcr.io/$GCP_PROJECT/$SERVICE:$VERSION\n\nbentoml build -f $YAML_PATH ./src/ --version $VERSION\nbentoml serve $SERVICE:latest --production\nbentoml containerize $SERVICE:$VERSION -t $IMAGE_URI\n</code></pre> <p>However, it's not handy to use the CLI if you want to do this step in a Python Script, and there is a lack of documentation on that. So I wrote a Python script to do this step, that can be used in a VertexAI component.</p> <p>Under the hood, when you do the <code>bentoml containerize</code> command, docker actually builds the image. If you want to use this as a VertexAI component, you cannot rely on Docker as you are already in a container.  The workaround here is to use Cloud Build to build the image.</p> <p>Check it out: ??? note \"workflows/build_bento.py\"</p> <pre><code>```python\nfrom loguru import logger\nimport bentoml\nfrom utils.bentoml import delete_bento_models_if_exists, save_model_to_bento, delete_bento_service_if_exists\nfrom utils.gcp import build_docker_image_with_cloud_build, upload_file_to_gcs\n\n\nPROJECT_ID = 'gcp_project_id'\n\n\ndef save_model_workflow(model_path: str, model_name: str) -&gt; str:\n    \"\"\"This function is used to save the model to BentoML and push it to GCR\n\n    It's an equivalent to these commands:\n    ```bash\n    bentoml build -f \"$YAML_PATH\" ./ --version $VERSION\n    bentoml containerize $SERVICE:$VERSION -t \"$IMAGE_URI\"\n    docker push \"$IMAGE_URI\"\n    ```\n\n    Args:\n        model_path (str): the path to the model artifact (eg. pickle).\n            Eg. \"gs://bucket-name/model_dirpath/\"\n        model_name (str): The name of the model. Eg. \"iris_classifier\"\n\n    Returns:\n        str: the URI of the pushed image\n            eg. \"gcr.io/project-id/iris-classifier:latest\"\n    \"\"\"\n    bento_filepath = \"path/to/bento/file/bento.yaml\"\n    service_name = f\"{model_name}_svc\"\n    delete_bento_models_if_exists(model_name)\n    save_model_to_bento(model_path, model_name)\n    logger.info(f\"Model saved: {bentoml.models.list()}\")\n    delete_bento_service_if_exists(service_name)\n\n    logger.info(f\"Building Bento service {service_name} from {bento_filepath}\")\n    bento_build = bentoml.bentos.build_bentofile(\n        bento_filepath,\n        build_ctx=\".\",\n        version=\"latest\",\n    )\n    logger.info(f\"Bento Service saved: {bentoml.bentos.list()}\")\n    service_name_tagged = f\"{bento_build.tag.name}:{bento_build.tag.version}\"\n    export_filename = f\"{service_name_tagged.replace(':', '_')}.zip\"\n    # `local_export_path` is the local file path where the Bento service is exported as a\n    # zip file. It is the output path where the exported Bento service is saved on the\n    # local machine before being uploaded to Google Cloud Storage (GCS).\n    local_export_path = bentoml.bentos.export_bento(\n        tag=service_name_tagged, path=f\"outputs/{export_filename}\", output_format=\"zip\"\n    )\n    logger.info(f\"Bento exported to {local_export_path}\")\n    export_gcs_uri = f\"{model_path}/{export_filename}\"\n    logger.info(f\"Uploading Bento to GCS to {export_gcs_uri}\")\n    upload_file_to_gcs(target_path=export_gcs_uri, local_path=local_export_path)\n    docker_image_uri = f\"europe-docker.pkg.dev/{PROJECT_ID}/eu.gcr.io/{service_name_tagged}\"\n    # Build Dockerfile of the Bento with cloud build, as an alternative to bentoml.container.build()\n    # which is not working with Vertex AI.\n    # the image is also pushed to the container registry (GAR)\n    build_docker_image_with_cloud_build(\n        export_gcs_uri,\n        docker_image_uri,\n        project_id=PROJECT_ID,\n        dockerfile_path=\"env/docker/Dockerfile\",  # Path to the Dockerfile in the Bento archive\n    )\n    logger.success(f\"Pushed docker image {docker_image_uri}\")\n    return docker_image_uri\n\n\nif __name__ == \"__main__\":\n    MODEL_PATH = 'path_to_model_artifact.pkl'\n    MODEL_NAME = 'iris_classifier'\n\n    docker_image_uri = save_model_workflow(\n        model_path=MODEL_PATH,\n        model_name=MODEL_NAME,\n    )\n```\n</code></pre>"},{"location":"howto_bentoml/#5-testing-the-service-locally","title":"5. Testing the service locally","text":"<p>On a terminal, launch the prediction service:</p> <pre><code>bentoml serve $SERVICE:$VERSION\n</code></pre> <p>Then, do a query:</p> <pre><code>import requests\nimport json\n\nquery = json.load(open(\"query.json\"))\n\nresponse = requests.post(\n    \"http://0.0.0.0:3000/classify\",\n    json=query,\n)\nprint(response.text)\n</code></pre> <p>Also, try to run the container:</p> <pre><code>docker run -it --rm -p 3000:3000 eu.gcr.io/$GCP_PROJECT_ID/$SERVICE:$VERSION serve --production\n</code></pre> <p>Then do the query again.</p> <p>If it doesn't work at this step, it won't work on VertexAI either. So make sure you fix the errors before going further.</p>"},{"location":"howto_bentoml/#6-upload-the-model-to-google-artifact-regitry-gar","title":"6. Upload the model to Google Artifact Regitry (GAR)","text":"<pre><code>docker push $IMAGE_URI`\n</code></pre> <p>Or in Python using Cloud Run:</p> <p>??? note \"utils/gcp.py\"</p> <pre><code>```python\nfrom google.cloud import storage\nfrom google.cloud.devtools import cloudbuild_v1\nfrom loguru import logger\n\n\ndef build_docker_image_with_cloud_build(\n    source_code_uri: str,\n    docker_image_uri: str,\n    project_id: str,\n    dockerfile_path: str = \"./Dockerfile\",\n):\n    \"\"\"This function build and push a docker image to GCR / Artifact registry using Cloud Build.\n    It's the equivalent of the CLI command: gcloud builds submit --tag $DOCKER_IMAGE_URI --file $DOCKERFILE_PATH $SOURCE_CODE_URI\n\n    Args:\n        source_code_uri (str): The archive containing the source code to build the docker image.\n            eg. gs://bucket-name/path/to/archive.tar.gz\n        docker_image_uri (str): The URI of the docker image to build. (--tag in docker build)\n            eg. europe-docker.pkg.dev/project-id/eu.gcr.io/image-name:tag\n        project_id (str): The project id where the Cloud Build job will run.\n        dockerfile_path (str): The path to the dockerfile to use to build the docker image. (--file in docker build)\n            eg. \"env/docker/Dockerfile\"\n    \"\"\"\n    logger.info(f\"Building docker image {docker_image_uri} using cloud build\")\n    # parsing the source code uri to get the bucket name and blob name\n    bucket_name, blob_name = (\n        storage.Blob.from_string(source_code_uri).bucket.name,\n        storage.Blob.from_string(source_code_uri).name,\n    )\n    client = cloudbuild_v1.CloudBuildClient()\n    storage_source = cloudbuild_v1.StorageSource(bucket=str(bucket_name), object=str(blob_name))\n    source = cloudbuild_v1.Source(storage_source=storage_source)\n    build = cloudbuild_v1.Build(\n        source=source,\n        steps=[\n            {\n                \"name\": \"gcr.io/cloud-builders/docker\",\n                \"args\": [\n                    \"build\",\n                    \"-t\",\n                    docker_image_uri,\n                    \"-f\",\n                    dockerfile_path,  # position of the Dockerfile in the Bento directory\n                    \".\",\n                ],\n            }\n        ],\n        images=[docker_image_uri],\n    )\n    request = cloudbuild_v1.CreateBuildRequest(project_id=project_id, build=build)\n    operation = client.create_build(request=request)\n    response = operation.result()\n    logger.info(f\"Build response: {response}\")\n\n\ndef upload_file_to_gcs(target_path: str, local_path: str) -&gt; None:\n    storage_client = storage.Client()\n\n    bucket_name, blob_name = (\n        storage.Blob.from_string(target_path).bucket.name,\n        storage.Blob.from_string(target_path).name,\n    )\n\n    bucket = storage_client.bucket(bucket_name)\n    logger.debug(f\"Saving File to {target_path}\")\n    blob = bucket.blob(blob_name)\n    blob.upload_from_filename(local_path)\n```\n</code></pre>"},{"location":"howto_bentoml/#7import-image-to-vertexai-model-registry","title":"7.Import image to VertexAI model registry","text":"<p>Now that your image is in Google Artifact Registry, you can import it to VertexAI model registry.</p> <p>Pay attention to:</p> <ul> <li>the predict route, it must be the same as your service (<code>/classify</code> in this example).</li> <li>the port, by default BentoML uses 3000 so stick to that</li> </ul> <p>Here is a bash script to do this step:</p> <pre><code>PREDICT_ROUTE=\"/classify\"\nHEALTH_ROUTE=\"/healthz\"\nPORTS=3000\n\necho \"Import as VertexAI model\"\nMODEL_ID=\"$(gcloud ai models list --region $LOCATION --filter=\"DISPLAY_NAME: ${MODEL_NAME}\" --format=\"value(MODEL_ID)\")\"\nif [ -z \"${MODEL_ID:=}\" ]; then\n  echo \"No existing model found for ${MODEL_NAME}. Importing model.\"\n  gcloud ai models upload \\\n    --region=$LOCATION \\\n    --display-name=$MODEL_NAME \\\n    --container-image-uri=$IMAGE_URI \\\n    --container-ports=$PORTS \\\n    --container-health-route=$HEALTH_ROUTE \\\n    --container-predict-route=$PREDICT_ROUTE \\\n    --project=$GCP_PROJECT\nelse\n  echo \"Existing model found for ${MODEL_NAME} (${MODEL_ID}). Importing new version\"\n  gcloud ai models upload \\\n    --region=$LOCATION \\\n    --display-name=$MODEL_NAME \\\n    --container-image-uri=$IMAGE_URI \\\n    --project=$GCP_PROJECT \\\n    --container-ports=$PORTS \\\n    --container-health-route=$HEALTH_ROUTE \\\n    --container-predict-route=$PREDICT_ROUTE \\\n    --parent-model=projects/${GCP_PROJECT}/locations/${LOCATION}/models/${MODEL_ID}\nfi\n</code></pre> <p>And the Python Equivalent:</p> <p>??? note \"workflows/push_model.py\"</p> <pre><code>```python\nimport os\nfrom utils.vertexai import get_model_if_exists, upload_model_to_registry\n\nIMAGE_TAG = os.getenv(\"IMAGE_TAG\", \"latest\")\nPREDICT_ROUTE = \"/classify\"\nHEALTH_ROUTE = \"/healthz\"\nPORTS = 3000\n\n\ndef push_model_workflow(\n    model_name: str,\n    serving_container_image_uri: str,\n) -&gt; str:\n    \"\"\"This workflow pushes model from a Google Artifact Registry to Vertex AI Model registry.\n    If the model already exists, it will be updated.\n\n    Args:\n        model_name (str): Name of the display model in Google Registry.\n        serving_container_image_uri (str): URI of the image in Google Artifact Registry.\n            Eg. \"eu.gcr.io/gcp-project-id/iris_classifier_svc:latest\"\n    \"\"\"\n    model = get_model_if_exists(model_name)\n    model = upload_model_to_registry(\n        model_name,\n        serving_container_image_uri,\n        parent_model=model.resource_name,\n        serving_container_predict_route=PREDICT_ROUTE,\n        serving_container_health_route=HEALTH_ROUTE,\n        serving_container_ports=[PORTS],\n        description=\"Product classification model, deployed automatically with Vertex AI\",\n        labels={\"image_tag\": os.getenv(\"IMAGE_TAG\", \"latest\")},\n        is_default_version=False,    # it safer to perform evaluation and make sure the model is good before setting the model as default\n    )\n    return model.display_name\n\n\nif __name__ == \"__main__\":\n    MODEL_NAME = \"iris_classifier\"\n    PROJECT_ID = \"gcp_project_id\"\n    SERVICE_NAME = f\"{MODEL_NAME}_svc\"\n    SERVING_CONTAINER_IMAGE_URI = (\n        f\"europe-docker.pkg.dev/{PROJECT_ID}/eu.gcr.io/{SERVICE_NAME}:{IMAGE_TAG}\"\n    )\n    push_model_workflow(MODEL_NAME, SERVING_CONTAINER_IMAGE_URI)\n```\n</code></pre>"},{"location":"howto_bentoml/#8deploy-model-to-vertexai-endpoint","title":"8.Deploy model to VertexAI endpoint","text":"<p>The final step is to deploy the model to a VertexAI \"online prediction\" endpoint.</p> <p>To do it manually, follow the following step. To do it programatically, check out the Python script.</p> <p>Manual steps:</p> <ol> <li>Go to VertexAI model registry</li> <li>Click on the model you want to deploy</li> <li>Click on the burger menu at the right on the latest version, then click on \"Set as Default\"</li> <li>Go to the VertexAI endpoint page and select the endpoint you want to deploy the model to</li> <li>Select the model you want to undeploy, and click on the burger menu at the right, then click on \"Undeploy model from endpoint\"</li> <li>Click on the burger menu at the right on the latest version, then click on \"Deploy to Endpoint\"</li> <li>\"Add to existing endpoint\" and select the endpoint</li> <li>Set traffic split to 100%, and define machine type</li> <li>Deploy</li> </ol> <p>It takes approximatively 15 minutes. Then you get an email. If the deployment failed, you get an email (Object: \"Vertex AI was unable to deploy model\") with a link to the stackdriver logs. To find the error in the logs, filter on severity=\"error\".</p> <p>You will have to fix the error, then go thought the whole process again.</p> <p>To know more, check out the official documentation.</p>"},{"location":"howto_bentoml/#9test-the-endpoint","title":"9.Test the endpoint","text":"<pre><code># Get the endpoint ID from the endpoint name\nENDPOINT_NAME=\"iris_classifier_endpoint\"\nENDPOINT_ID=$(gcloud ai endpoints list --filter DISPLAY_NAME=$ENDPOINT_NAME --region=$LOCATION --format=\"value(ENDPOINT_ID)\")\n\n# OR uncomment the following line to set the endpoint ID manually:\n# ENDPOINT_ID=\"123456789\"\n\nINPUT_DATA_FILE=\"query.json\"\n\necho \"Sendind a request to the endpoint ${ENDPOINT_NAME} with ID: ${ENDPOINT_ID}\"\n\ncurl \\\n-X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json\" \\\nhttps://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/${ENDPOINT_ID}:predict \\\n-d \"@${INPUT_DATA_FILE}\"\n</code></pre>"},{"location":"howto_conditional_components/","title":"How to condition the execution of a component?","text":"<p>You can control whether a component is executed or not using a <code>dsl.Condition</code>.</p> <p>In the following example, <code>flip_coin</code> and <code>my_comp</code> are both component objects.</p> <pre><code>@dsl.pipeline\ndef my_pipeline():\n    coin_flip_task = flip_coin()\n    with dsl.Condition(coin_flip_task.output == 'heads'):\n        conditional_task = my_comp()\n</code></pre>"},{"location":"howto_conditional_components/#reference","title":"Reference","text":"<ul> <li>Vertex notebook example</li> <li>Kubeflow control flow doc</li> </ul>"},{"location":"howto_gpu/","title":"How to set up a GPU Docker container that runs","text":"<p>While training Deep Learning algorithms, you might need GPUs to speed up the process. This tutorial will show you how to:</p> <ul> <li>Create a Docker Image with GPU support</li> <li>Set up Vertex AI to run the Docker Image on a GPU</li> <li>Make sure the Vertex training component step access the GPU</li> </ul>"},{"location":"howto_gpu/#create-a-docker-image-with-gpu-support","title":"Create a Docker Image with GPU support","text":"<p>Please refer to this excellent tutorial to create a Docker Image with GPU support.</p>"},{"location":"howto_gpu/#1-identify-the-cuda-version-of-your-local-environment","title":"1. Identify the CUDA version of your local environment","text":"<p>This take-away from the article is important:</p> <p>[!IMPORTANT] Always use the same CUDA and cuDNN version in Docker image as present in the underlying host machine</p> <p>So the first step is to identify which CUDA version and PyTorch version you are using in your local environment.</p> <p>To identify the CUDA version of your local environment, run:</p> <pre><code>$ pip freeze | grep cu\n\nnvidia-cudnn-cu11==8.5.0.96\n</code></pre> <p>Means I have CUDA 11 installed, and CuDNN 8.</p>"},{"location":"howto_gpu/#2-select-a-docker-image-that-matches-your-cuda-version","title":"2. Select a Docker image that matches your CUDA version","text":"<p>Docker hub of Nvidia has a lot of images, so understanding their tags and selecting the correct image is the most important building block.</p> <p>Prefers <code>nvidia/cuda</code> Docker images compared to <code>pytorch/</code> and  <code>vertex-ai/training</code>.</p> <p>You need to select a Docker image that matches this version. Check out the Docker Hub to find a compatible version. In my case, I found this one: <code>nvidia/cuda:11.7.0-cudnn8-runtime-ubuntu20.04</code></p> <p>so I would start my dockerfile with:</p> <pre><code>FROM nvidia/cuda:11.7.0-cudnn8-runtime-ubuntu20.04\n</code></pre>"},{"location":"howto_gpu/#3-install-python","title":"3. Install Python","text":"<p>You need to install the same Python version as the one you are using in your local environment.</p> <p>In my case, I am using Python 3.9, so I would add this line to my Dockerfile:</p> <pre><code>FROM nvidia/cuda:11.7.0-cudnn8-runtime-ubuntu20.04\nENV PYTHON_VERSION=3.9\nENV CONTAINER_TIMEZONE=Europe/Paris\n\n# Set the timezone to prevent tzdata asking for user input\nRUN ln -snf /usr/share/zoneinfo/$CONTAINER_TIMEZONE /etc/localtime &amp;&amp; echo $CONTAINER_TIMEZONE &gt; /etc/timezone\n\nRUN apt update \\\n    &amp;&amp; apt install --no-install-recommends -y build-essential \\\n    tzdata \\\n    python${PYTHON_VERSION} \\\n    python${PYTHON_VERSION}-venv \\\n    python3-pip \\\n    python3-setuptools \\\n    python3-distutils \\ \n    &amp;&amp; apt clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN ln -s -f /usr/bin/python${PYTHON_VERSION} /usr/bin/python3 &amp;&amp; \\\n    ln -s -f /usr/bin/python${PYTHON_VERSION} /usr/bin/python &amp;&amp; \\\n    ln -s -f /usr/bin/pip3 /usr/bin/pip\n</code></pre>"},{"location":"howto_gpu/#4-install-the-pytorch-or-tensorflow-version-that-is-compatible-with-your-cuda-version","title":"4. Install the PyTorch or Tensorflow version that is compatible with your CUDA version","text":"<p>[!WARNING] Don\u2019t blindly install latest tensorflow/pytorch library from PyPi. It is absolutely incorrect that any version of this both package will work with any version of CUDA, cuDNN. In fact, the combination of the latest version of both, tensorflow/pytorch with CUDA/cuDNN may not be compatible</p> <p>For PyTorch for example, go to the Install Section to find a compatible version. You can also browse the previous versions here.</p> <p>You need to install a GPU version. In my case, I found this one: <code>torch==2.0.0+cu117</code> in the website, with the install instruction:</p> <pre><code>pip install torch==2.0.0+cu117 --index-url https://download.pytorch.org/whl/cu117\n</code></pre> <p>So I would add this line to my Dockerfile, with the <code>--no-cache-dir</code> option to avoid caching the wheel file, which can crash the build.</p> <p>Then, I also install the requirements.txt file of my project, and copy the source code.</p> <pre><code>FROM nvidia/cuda:11.7.0-cudnn8-runtime-ubuntu20.04\n\nARG PROJECT_ID\nENV PROJECT_ID=${PROJECT_ID}\nENV PYTHON_VERSION=3.9\nENV CONTAINER_TIMEZONE=Europe/Paris\n\n# Set the timezone to prevent tzdata asking for user input\nRUN ln -snf /usr/share/zoneinfo/$CONTAINER_TIMEZONE /etc/localtime &amp;&amp; echo $CONTAINER_TIMEZONE &gt; /etc/timezone\n\n# export DEBIAN_FRONTEND=noninteractive \\\nRUN apt update \\\n    &amp;&amp; apt install --no-install-recommends -y build-essential \\\n    tzdata \\\n    python${PYTHON_VERSION} \\\n    python${PYTHON_VERSION}-venv \\\n    python3-pip \\\n    python3-setuptools \\\n    python3-distutils \\ \n    &amp;&amp; apt clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN ln -s -f /usr/bin/python${PYTHON_VERSION} /usr/bin/python3 &amp;&amp; \\\n    ln -s -f /usr/bin/python${PYTHON_VERSION} /usr/bin/python &amp;&amp; \\\n    ln -s -f /usr/bin/pip3 /usr/bin/pip\n\nRUN python3 -m pip install --no-cache-dir torch==2.0.0+cu117 --index-url https://download.pytorch.org/whl/cu117\nCOPY requirements.txt .\nRUN python3 -m pip install --no-cache-dir --upgrade -r requirements.txt\n\nCOPY . .\n</code></pre>"},{"location":"howto_gpu/#ask-for-a-gpu-in-your-vertex-pipeline-for-your-training-step","title":"Ask for a GPU in your Vertex pipeline, for your training step","text":"<p>In your <code>pipeline.py</code>, add <code>set_gpu_limit</code> and <code>add_node_selector_constraint</code> to your training component, then select a GPU type:</p> <pre><code>@kfp.dsl.pipeline(name=\"training-pipeline\")\ndef pipeline(\n    project_id: str,\n):\n    split_data_task = load_and_split_data_component(\n        project_id=project_id,\n    )\n    train_model_task = (\n        (\n            train_model_component(\n                project_id=project_id,\n                train=split_data_task.outputs[\"train_dataset\"],\n                val=split_data_task.outputs[\"val_dataset\"],\n                test=split_data_task.outputs[\"test_dataset\"],\n            )\n        )\n        .set_gpu_limit(1)\n        .add_node_selector_constraint(\n            label_name=\"cloud.google.com/gke-accelerator\",\n            value=\"NVIDIA_TESLA_P100\",\n        )\n    )\n</code></pre> <p>Troubleshooting errors</p> <ul> <li>Make sure the Quotas allows you to query the GPU type you want. You might have to request a quota increase.</li> <li>Also, you must select a GPU type that is available in the region you are using.</li> </ul>"},{"location":"howto_gpu/#run-your-pipeline-and-make-sure-the-vertex-training-component-step-access-the-gpu","title":"Run your pipeline and make sure the Vertex training component step access the GPU","text":"<p>In the code of my component/ workflow, I log the GPU availability with:</p> <pre><code>import torch\n\ndef train_model_workflow(\n    some_parameters: int,\n):\n    logger.info(f\"GPU:{torch.cuda.is_available()}\")\n</code></pre> <p>This allows me to check is the GPU is available in the logs of the Vertex training component step.</p> <p>If the GPU is available, you will see this in the logs:</p> <pre><code>GPU:True\n</code></pre> <p>If the GPU is not available, it means your Docker image is not configured properly, or you did not select a GPU type in your Vertex pipeline. Iterate on the previous steps to fix the issue.</p>"},{"location":"howto_pass_complex_objects/","title":"How to pass a complex object between components?","text":"<p>Vertex components only allow you to use <code>dict</code>, <code>list</code>, <code>int</code>, <code>float</code>, <code>string</code>, and <code>bool</code> as input and output parameters. </p> <p>You are probably going to want to pass more complex, or custom types to your components. For this you will need to pass them using Artifacts.</p>"},{"location":"howto_pass_complex_objects/#artifacts","title":"Artifacts","text":"<p>Although you can not directly pass complex types directly, components can produce Artifacts as outputs that may then be used as inputs in other components.</p> <p>Save and load an artifact</p> <pre><code>@component\ndef first_component(artifact: Output[Artifact]):\n    # Component logic that creates data here\n    data = \"...\"\n    with open(artifact.path, \"w+\") as output_file:\n        artifact_contents = output_file.write(data)\n</code></pre> <pre><code>@component\ndef second_component(artifact: Input[Artifact]):\n    with open(artifact.path, \"r\") as input_file:\n        artifact_contents = input_file.read()\n        print(f\"artifact contents: {artifact_contents}\")\n    # Component logic that uses data here\n</code></pre> <p>The <code>Artifact</code> type is generic and flexible. There are more specialized types (<code>Model</code>, <code>Dataset</code>, <code>Markdown</code>, ...) that you should use when relevant. Artifact types</p> <p>If you need to pass custom types, consider saving then as a pickle file.</p>"},{"location":"howto_pass_complex_objects/#references","title":"References","text":"<ul> <li>lightweight_functions_component_io_kfp</li> </ul>"},{"location":"howto_schedule_pipelines/","title":"How to schedule pipelines?","text":"<p>Your business needs may require you to run your pipelines every month, week, or day. This is not directly supported in Vertex pipelines yet.</p>"},{"location":"howto_schedule_pipelines/#use-the-dedicated-skaff-accelerator","title":"Use the dedicated Skaff accelerator","text":"<p>https://artefact.roadie.so/catalog/default/component/scheduled-pipelines</p>"},{"location":"howto_schedule_pipelines/#do-it-yourself","title":"Do it yourself","text":"<p>Use Cloud Scheduler and a Cloud function to run a pre-compiled pipeline.</p> <p>This is the GCP recommended way. Compile your pipelines locally or in your CI/CD, and store the json output somewhere on GCP. Set up a job on Cloud Scheduler to trigger a Cloud Function (or Cloud Run) to load a compiled pipeline and run it.</p> <p>There is no Artefact accelerator for this at the moment, but one will be available some time soon\u2122\ufe0f.</p> <p>Follow the docs to deploy that yourself.</p>"},{"location":"managing_packages/","title":"Managing python packages and dependencies","text":"<p>Assuming you are working with function-based components only and one docker base image, there are two ways for you to make python packages available in components.</p>"},{"location":"managing_packages/#adding-globally-available-packages","title":"Adding globally available packages.","text":"<p>As all the components use the same base docker image, installing a package there will make it available in your code in all the components.</p> <p>So, to add packages, do the following:</p> <ul> <li>Add whatever packages + version constraints you want in the <code>requirements.txt</code> file used to build the docker base image</li> <li>Rebuild your docker image with the new requirements (<code>make build_image</code>)</li> </ul> <p>This is the preferred way of doing things in most situations. It ensures that your dependencies versions are consistent in your local env and across components and pipelines, reducing compatibilities issues. </p> <p>Having unused packages in a component my sound sub-optimal, but it is generally not an issue.</p> <p>However, if you need a very cumbersome library for a specific component (e.g. pytorch, tensorflow, or other) you can surgically insert it like so:</p>"},{"location":"managing_packages/#adding-a-package-in-a-single-component","title":"Adding a package in a single component.","text":"<p>This method has a few drawbacks. It creates additional overhead when starting the component due to the time it takes to install the dependency. It will also cause pipelines to fail if package installation fails for whatever reason. This is less desirable than failing fast at the docker image building step. Finally, overusing this will also decrease the overall coherence of your codebase and running different steps with different packages/version may have undesired consequences.</p> <p>In a component file:</p> <p>my_component.py</p> <pre><code>from kfp.dsl import component\n\n@component(\n    base_image=f\"europe-west1-docker.pkg.dev/{os.getenv(\"PROJECT_ID\")}/vertex-pipelines-docker/vertex-pipelines-base:latest\",\n    packages_to_install=[\"torch==2.0.0\"]  # Installs pytorch specifically for this component.\n)\ndef my_component():\n    # Do things\n    pass\n</code></pre>"},{"location":"managing_pipeline_configuration/","title":"Managing pipelines configurations","text":"<p>It is a very good practice to make your pipelines parameterizable. That means replacing all hardcoded values by variables that you are going to pass to the pipeline and eventually to the components.</p> <p>Pipeline parameters can be:</p> <ul> <li>Model training parameters</li> <li>Start date and end date of the data you want to work with</li> <li>Product categories</li> <li>Geography (country, region, stores, ...)</li> <li>Customer segments</li> <li>...</li> </ul> <p>Leveraging parametrized pipelines will allow you to run the same pipeline with different parameter sets. This is much more practical to deploy and maintain compared to pipelines with slightly different hardcoded values.</p>"},{"location":"managing_pipeline_configuration/#passing-config-values-to-the-pipeline","title":"Passing config values to the pipeline","text":""},{"location":"managing_pipeline_configuration/#basic-pipeline-parametrization","title":"Basic pipeline parametrization","text":"<p>In most cases, just passing values to your pipeline as parameters is the simplest and best way to go.</p> <pre><code>import os\n\nimport kfp\nfrom kfp import compiler\nimport google.cloud.aiplatform as aip\nfrom kfp.dsl import component\n\n\n@component(base_image=f'europe-west1-docker.pkg.dev/{os.getenv(\"PROJECT_ID\")}/vertex-pipelines-docker/vertex-pipelines-base:latest')\ndef dummy_task(project_id: str, country: str, start_date: str, end_date: str):\n    pass\n\n\n# This part defines the pipeline and its parameters\n@kfp.dsl.pipeline(name=\"parametrized-pipeline\")\ndef pipeline(project_id: str, country: str, start_date: str, end_date: str):\n    dummy_task(\n        project_id=project_id,\n        country=country,\n        start_date=start_date,\n        end_date=end_date\n    )\n\n\n# This part compiles the pipeline and runs it\nif __name__ == '__main__':\n    PROJECT_ID = os.getenv(\"PROJECT_ID\")\n    PIPELINE_NAME = \"parametrized-pipeline\"\n    BUCKET_NAME = f\"gs://vertex-{PROJECT_ID}\"\n    SERVICE_ACCOUNT = f\"vertex@{PROJECT_ID}.iam.gserviceaccount.com\"\n\n    compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"./pipeline.json\")\n    aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)\n\n    job = aip.PipelineJob(\n        display_name=PIPELINE_NAME,\n        template_path=\"pipeline.json\".replace(\" \", \"_\"),\n        pipeline_root=f\"{BUCKET_NAME}/root\",\n        location=\"europe-west1\",\n        enable_caching=False,\n\n        # Parameters values are passed from here to the pipeline\n        parameter_values={\n            \"project_id\": PROJECT_ID,\n            \"country\": \"france\",\n            \"start_date\": \"2022-01-01\",\n            \"end_date\": \"2022-12-31\"\n        },\n    )\n\n    job.run(service_account=SERVICE_ACCOUNT)\n</code></pre> <p>The parameters will be clearly displayed in the UI: </p> <p>Making all your parameters explicit as pipeline and component arguments represents some overhead in terms of code but for us it is the best solution as it makes all parameters visible in the UI and allows people who read the code to see very easily how each parameter is defined</p> <p>Alternative options that are possible (that we generally do not recommend!):  - Importing parameters directly in lib or component files =&gt; this bypasses the vertex UI and can make debugging much harder - Having a config dictionary and passing it as an arguments in pipeline / components. This solution is less bad and can actually have some application to pass on very large or well defined configs (ex: model parameters to pass directly to Catboost). Config will be visible in the UI but not in an easy to read way.  </p>"},{"location":"managing_pipeline_configuration/#dynamically-loading-pipeline-parameters","title":"Dynamically loading pipeline parameters","text":"<p>Dynamically loading a config in the pipeline</p> <p>Pipeline parameters values are rendered when passed to components. That means you can not easily pass a configuration name and load it in the pipeline body itself.</p> <pre><code>@kfp.dsl.pipeline(name=\"parametrized-pipeline\")\ndef pipeline(config_name: str):\n    print(config_name)  # Result: {{pipelineparam:op=;name=config_name}} -&gt; not rendered\n</code></pre> <p>You would need a dedicated component to load your configuration and then output the values to downstream tasks. This is very complex for no benefits and probably not worth it.</p> <p>Instead, load the values before compiling the pipeline</p> <pre><code>job = aip.PipelineJob(\n    display_name=PIPELINE_NAME,\n    template_path=\"pipeline.json\".replace(\" \", \"_\"),\n    pipeline_root=f\"{BUCKET_NAME}/root\",\n    location=\"europe-west1\",\n    enable_caching=False,\n\n    parameter_values=load_config(\"config_1\"),\n)\n</code></pre> <p>??? info \"<code>load_config</code> function\"</p> <pre><code>This function loads configuration values from a file as a `dict`.\n````python3\ndef load_config(config_name: str) -&gt; Dict:\n    with open(Path(__file__).parent.parent / \"configs\" / f\"{config_name}.json\") as f:\n        config = json.load(f)\n    return config\n````\n</code></pre> <p>??? info \"<code>config_1.json</code>\"</p> <pre><code>This file contains the configuration we want to load.\n````json\n{\n  \"project_id\": \"ocmlf-vial-16\",\n  \"country\": \"france\",\n  \"start_date\": \"2022-01-01\",\n  \"end_date\": \"2022-12-31\"\n}\n````\n</code></pre>"},{"location":"managing_pipeline_configuration/#storing-configs","title":"Storing configs","text":"<p>As your project grows in scale, you will want a way to cleanly store your pipeline configurations. Here are a few suggestions for this</p>"},{"location":"managing_pipeline_configuration/#locally-in-a-markup-language-json-yaml-toml-etc","title":"Locally, in a markup language (json, yaml, toml, etc)","text":"<p>Storing configurations this way is simple and works well. Write one configuration per file. Then you can easily load the right one as a python dict and pass it to your pipeline.</p>"},{"location":"managing_pipeline_configuration/#locally-in-python-files","title":"Locally, in python files","text":"<p>You can also centralize your parameters in python files and import them to your pipeline. The issue with that approach is you are going to need to store all your configs in the same file, which can get messy. </p> <p>If you want to split the configs, you will either need to: </p> <ul> <li>Manually import them all in your pipeline file. This creates a tight coupling between your config files and your pipelines, which is undesirable.</li> <li>Dynamically import them. This is complex, and you do not get autocompletion anymore.</li> </ul> <p>Basically, this is only a good option when you know you will never have a lot of different configs. And even then the benefits are fairly small.</p>"},{"location":"managing_pipeline_configuration/#remotely","title":"Remotely","text":"<p>If you need your configs to be centrally available on GCP you may want to store them remotely in a database. </p> <p>This can be useful when you need these configurations available elsewhere on GCP and not just in your pipeline. </p> <p>It is also practical if your configurations are not defined in the same codebase as your pipelines. For example, if you let users pilot pipelines via a Google sheet or a Streamlit, etc...</p> <p>In this case, using Firestore is a very good option. It requires very little setup, and is very easy to use since it stores data as documents (essentially dicts). You will also be able to go and view your stored configurations and their contents in the UI.</p> <p>??? example \"Firestore usage examples\"     !!! example \"Interacting with Firestore\"</p> <pre><code>    ````python3\n    from typing import Dict, Any, Union\n\n    from google.cloud import firestore\n\n\n    def set(collection: str, document_name: str, document_as_dict: Dict[str, Any]) -&gt; None:\n        client = firestore.Client()\n        client.collection(collection).document(document_name).set(document_as_dict)\n\n\n    def get(collection: str, document: str) -&gt; Union[Dict[str, Any], None]:\n        client = firestore.Client()\n        doc_ref = client.collection(collection).document(document)\n        doc = doc_ref.get().to_dict()  # type: Union[Dict[str, Any], None]\n        return doc\n\n\n    if __name__ == '__main__':\n        # Add a config in Firestore\n        set(\n            collection=\"Pipeline1\",\n            document_name=\"config2\",\n            document_as_dict={\n                \"project_id\": \"ocmlf-vial-16\",\n                \"country\": \"france\",\n                \"start_date\": \"2022-01-01\",\n                \"end_date\": \"2022-12-31\"\n            }\n        )\n\n        # Fetch a config from Firestore\n        conf = get(\"Pipeline1\", \"config1\")\n        print(conf)  # {'end_date': '2022-12-31', 'country': 'france', 'project_id': 'ocmlf-vial-16', 'start_date': '2022-01-01'}\n    ````\n\n![](assets/firestore_config.png)\n</code></pre> <p>You can also store configurations as files on a bucket, but that is slightly less practical.</p>"},{"location":"parallelizing_processing/","title":"Parallelizing processings in pipelines","text":"<p>There are three ways to implement parallelization in Vertex Pipelines: single component multiprocessing, multiple parallel components, and multiple pipelines. </p>"},{"location":"parallelizing_processing/#multiple-component-multiprocessing-recommended","title":"Multiple component multiprocessing (recommended)","text":"<p>Parallelizing processing in vertex pipelines by executing a component multiple times with different parameters usually works well with no additional overhead, and few surprises.</p> <pre><code>@component(base_image=f'europe-west1-docker.pkg.dev/{os.getenv(\"PROJECT_ID\")}/vertex-pipelines-docker/vertex-pipelines-base:latest')\ndef dummy_task():\n    pass\n\n\n@kfp.dsl.pipeline(name=\"fan-out-fan-in\")\ndef pipeline():\n    first_task = dummy_task()\n\n    parallel_tasks = []\n    for _ in range(3):\n        parallel_task = dummy_task()\n        parallel_task.after(first_task)\n        parallel_tasks.append(parallel_task)\n\n    final_task = dummy_task()\n    for task in parallel_tasks:\n        final_task.after(task)\n</code></pre> <p>Produces the following pipeline: </p>"},{"location":"parallelizing_processing/#single-component-multiprocessing","title":"Single component multiprocessing","text":"<p>You can implement multiprocessing in python within a component to speed up its execution.</p> <p>Multiprocessing vs Threading vs AsyncIO in python</p> <p>There are multiple parallelization methods in python. Make sure you pick the right one for your use-case. </p> <p>??? example \"Python parallelization with multiprocessing.Pool\"</p> <pre><code>````python3\nfrom multiprocessing import Pool\nfrom random import randint\nfrom time import sleep\n\n\ndef execute_task(arg1, arg2):\n    print(f\"Executing task with args {arg1, arg2}\")\n    result = randint(arg1, arg2)\n    sleep(result)\n    return result\n\n\nif __name__ == '__main__':\n    tasks = [(1, 2), (3, 4), (5, 6)]\n\n    with Pool(processes=2) as pool:\n        result = pool.starmap(execute_task, tasks)\n\n    print(f\"Results: {result}\")\n````\n</code></pre> <p>Multiprocessing on vCPUs (CPUs in the cloud) may not behave like it would on your local machine. Different vCPUs may actually belong to the same hardware CPU, negating any performance benefits.</p>"},{"location":"parallelizing_processing/#multiple-pipelines","title":"Multiple pipelines","text":"<p>You can also run multiple pipelines with different parameters. That is a useful approach if your whole pipeline depends on a few global parameters like a product category, location, customer group, etc. The downside of this however is that it is going to be difficult to exchange information between pipelines, so avoid this approach unless you are sure your various pipeline will not need to.</p>"},{"location":"splitting_components/","title":"Rules of thumb to split components","text":"<p>It can be difficult to decide how to split your pipeline into components. Overall, components should aim to be a self-contained business logic unit with minimal inputs and outputs.</p> <p>The components in <code>vertex/pipelines/my_first_pipeline.py</code> are very small.  In practice for such a simple pipeline a single component would be better, but the goal here is to illustrate how  to pass parameters within a pipeline.</p>"},{"location":"splitting_components/#when-to-split-a-component","title":"When to split a component","text":"<p>Having a big monolithic pipeline made of one component is obviously not ideal. </p> <p>To iterate faster: if you find yourself waiting for a lot of code to execute before the execution flow gets to your actual changes, then it's probably a good reason to split your component. For example, if you changed some model training parameters and need to wait for a data preprocessing step for 10 minutes before your model is retrained, split the two steps and save the training dataset as an artifact.</p> <p>To leverage orchestration: parallelization can be achieved fairly easily in pipelines. If some processing is easily splittable, don't hesitate to use this at your advantage for a faster pipeline. If you are trying to find the right hyper-parameters for a training, you could have one component by hyper-parameter set:</p> <p>Splitting a grid search between components</p> <pre><code>@kfp.dsl.pipeline(name=\"find-best-hyper-parameters\")\ndef pipeline(project_id: str, input_table: str):\n    load_data_task = load_training_data(project_id=project_id, gcp_region=\"europe-west1\", input_table=input_table)\n\n    hyper_parameters_to_test = range(10)\n    grid_search_results = []\n    for hyper_parameter in hyper_parameters_to_test:\n        result = train_and_evaluate(hyper_parameter, load_data_task.outputs[\"training_data\"])\n        grid_search_results.append(str(result.outputs[\"result\"]))\n\n    save_best_model(grid_search_results)\n</code></pre> <p>In the UI: </p>"},{"location":"splitting_components/#when-to-merge-two-components","title":"When to merge two components","text":"<p>On the other end, it is not ideal either to have micro-components that barely do anything. You are going to encounter performance issues in your pipeline due to the overhead for component initialization, as well as having to manage a lot of tedious artifact management to pass data around your pipeline.</p> <p>If you find yourself making changes on 4+ components to add a feature, you should probably merge some of them.</p>"},{"location":"when_to_use_premade_components/","title":"Using premade components","text":"<p>There are a few pre-made Vertex AI components available as python packages through the google-cloud-pipeline-components SDK.</p> <ul> <li>Homepage</li> <li>Actual docs</li> </ul> <p>They provide component interfaces for the rest of the Vertex AI platform, BQ ML, or AutoML that makes it easier to integrate them in pipelines. These can be used to quickly put together a walking skeleton. However, be aware of some limitations when using these pre-made components.</p> <ul> <li>They are not executable locally. This means that iteration time using these components will be fairly long compared to a homemade implementation.</li> <li>The level of abstraction they provide is small. They typically only wrap an already existing GCP API method in a component.</li> <li>This introduces black boxes in your pipeline. It will be more difficult to dive into the internals to debug.</li> <li>They're not very well documented.</li> </ul> <p>Given this limitations we do not recommend using the components that only serve as wrapper for a well functioning python API (hence replacing only few lines of codes) as for those cases we feel the loss of not being able to run locally is not worth it compare to the gains. </p> <p>However for some uses cases (ex: batch predictions), pre-made components can actually save you a lot of time so knowing about these components, and potentially integrating them in your pipelines can be a great idea and save some precious time despite their limitations.  Make sure you read their docs thoroughly to ensure they do what you need them to and limit integration risks.</p>"}]}